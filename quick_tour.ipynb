{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mindspore as ms\n",
    "\n",
    "ms.set_seed(1)\n",
    "ms.context.set_context(mode=ms.context.GRAPH_MODE, device_target='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train ResNet18 on cifar10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare dataset \n",
    "\n",
    "Download cifar10 dataset, extract it, and put it in `cifar10_dir` folder. \n",
    "Then run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\ndata = next(loader_train.create_dict_iterator())\\ndata['image'][0]\\nplt.imshow(.asnumpy().squeeze(), cmap=plt.cm.gray)\\nplt.title(data['label'][0].asnumpy(), fontsize=20)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dataset\n",
    "from mindcv.data import create_dataset, create_transforms, create_loader\n",
    "\n",
    "# create dataset\n",
    "cifar10_dir = '/data/cifar/cifar-10-batches-bin/'\n",
    "num_classes = 10\n",
    "num_workers = 8\n",
    "\n",
    "dataset_train = create_dataset(name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers, download=False)\n",
    "\n",
    "# create transform and get trans list\n",
    "trans = create_transforms(dataset_name='cifar10', image_resize=224)\n",
    "\n",
    "# get data loader for training\n",
    "loader_train = create_loader(\n",
    "        dataset=dataset_train,\n",
    "        batch_size=32,\n",
    "        is_training=True,\n",
    "        num_classes=num_classes,\n",
    "        transform=trans,\n",
    "        num_parallel_workers=num_workers,\n",
    "    )\n",
    "\n",
    "# TODO: visualize \n",
    "#data = next(loader_train.create_dict_iterator())\n",
    "#print(data['image'][0].asnumpy().squeeze())\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "data = next(loader_train.create_dict_iterator())\n",
    "data['image'][0]\n",
    "plt.imshow(.asnumpy().squeeze(), cmap=plt.cm.gray)\n",
    "plt.title(data['label'][0].asnumpy(), fontsize=20)\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network and train\n",
    "from mindcv.models import create_model\n",
    "from mindcv.loss import create_loss\n",
    "from mindcv.optim import create_optimizer\n",
    "from mindcv.scheduler import create_scheduler\n",
    "\n",
    "# build resnet model\n",
    "network = create_model(model_name='resnet18', num_classes=num_classes, pretrained=False)\n",
    "\n",
    "# set loss function\n",
    "loss = create_loss(name='CE')\n",
    "\n",
    "# set optimizer \n",
    "steps_per_epoch = loader_train.get_dataset_size()\n",
    "opt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563\n",
      "epoch: 1 step: 10, loss is 2.543825387954712\n",
      "epoch: 1 step: 20, loss is 2.4227259159088135\n",
      "epoch: 1 step: 30, loss is 2.21061110496521\n",
      "epoch: 1 step: 40, loss is 2.31510853767395\n",
      "epoch: 1 step: 50, loss is 2.1939709186553955\n",
      "epoch: 1 step: 60, loss is 2.1772873401641846\n",
      "epoch: 1 step: 70, loss is 1.968402624130249\n",
      "epoch: 1 step: 80, loss is 2.1735341548919678\n",
      "epoch: 1 step: 90, loss is 2.1211137771606445\n",
      "epoch: 1 step: 100, loss is 2.060058116912842\n",
      "epoch: 1 step: 110, loss is 2.3186118602752686\n",
      "epoch: 1 step: 120, loss is 1.942218542098999\n",
      "epoch: 1 step: 130, loss is 1.8849071264266968\n"
     ]
    }
   ],
   "source": [
    "# TODO: simplify the training code \n",
    "\n",
    "from mindspore import FixedLossScaleManager, Model, LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "ckpt_save_dir = './ckpt'\n",
    "\n",
    "model = Model(network, loss_fn=loss, optimizer=opt, metrics={'acc'})\n",
    "print(steps_per_epoch)\n",
    "loss_cb = LossMonitor(per_print_times=10)\n",
    "time_cb = TimeMonitor(data_size=10)\n",
    "callbacks = [loss_cb, time_cb]\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=steps_per_epoch)\n",
    "ckpt_cb = ModelCheckpoint(prefix='resnet18_sratch',\n",
    "                          directory=ckpt_save_dir,\n",
    "                          config=ckpt_config)\n",
    "callbacks.append(ckpt_cb)\n",
    "\n",
    "# train model\n",
    "#sink_mode = not (target_device == \"CPU\")\n",
    "model.train(10, loader_train, callbacks=callbacks, dataset_sink_mode=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgraph",
   "language": "python",
   "name": "xgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
